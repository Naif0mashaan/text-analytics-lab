# -*- coding: utf-8 -*-
"""Untitled7.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1OkzPasNQ0t-4YZ1jAoYRc_6OBvoTSoFj
"""

!pip -q install beautifulsoup4

import requests
from bs4 import BeautifulSoup

import requests
from bs4 import BeautifulSoup

url = "https://dataquestio.github.io/web-scraping-pages/simple.html"
res = requests.get(url)

print("Status:", res.status_code)
print(res.text[:300])  # معاينة أول جزء من HTML (اختياري)

soup = BeautifulSoup(res.text, "html.parser")

p_text = soup.find("p").get_text(strip=True)
print("Extracted P:", p_text)

with open("extracted_simple.txt", "w", encoding="utf-8") as f:
    f.write(p_text)

print("Saved: extracted_simple.txt")

url = "https://sites.google.com/view/cisb5123/home"
page = requests.get(url)



print(page.status_code)

data = BeautifulSoup(page.content, "html.parser")

h2_tag = data.find("h2")

if h2_tag:
    title = h2_tag.get_text(strip=True)
    print(f"Extracted title: {title}")
else:
    title = "Title not found"
    print(f"Extracted title: {title}")
    print("\n--- Debugging: No h2 tag found. Showing a snippet of the parsed HTML ---\n")
    print(data.prettify()[:2000]) # Print first 2000 characters of parsed HTML for inspection

h2_tag = data.find("h2")

if h2_tag:
    # Based on manual inspection, the text is directly in <h2>, not a <span> inside <h2>
    subtitle = h2_tag.get_text(strip=True)
    print(f"Extracted subtitle: {subtitle}")
else:
    subtitle = "Subtitle not found. This might be due to dynamic content loading."
    print(f"Extracted subtitle: {subtitle}")
    print("Hint: The <h2> tag was not found in the static HTML. The content might be loaded via JavaScript.")
    print("Consider using a tool like Selenium for scraping dynamic web pages.")

print(data.prettify()[:2000])

import requests
from bs4 import BeautifulSoup
import csv

url = "https://quotes.toscrape.com"
response = requests.get(url)

print("Status:", response.status_code)

soup = BeautifulSoup(response.text, "html.parser")

quotes = soup.find_all("div", class_="quote")

print("Number of quotes on first page:", len(quotes))

import requests
from bs4 import BeautifulSoup
import csv

url = "https://quotes.toscrape.com"
response = requests.get(url)
soup = BeautifulSoup(response.text, "html.parser")

quote_blocks = soup.find_all("div", class_="quote")

rows = []
for q in quote_blocks:
    text = q.find("span", class_="text").get_text(strip=True)
    author = q.find("small", class_="author").get_text(strip=True)
    tags = [t.get_text(strip=True) for t in q.find_all("a", class_="tag")]

    rows.append({
        "text": text,
        "author": author,
        "tags": ", ".join(tags)
    })

# تأكيد سريع
print("First row example:")
print(rows[0])
print("Total rows:", len(rows))

with open("quotes.csv", "w", newline="", encoding="utf-8") as f:
    writer = csv.DictWriter(f, fieldnames=["text", "author", "tags"])
    writer.writeheader()
    writer.writerows(rows)

print("Saved file: quotes.csv")

import requests
from bs4 import BeautifulSoup
import csv

base_url = "https://quotes.toscrape.com"
current_url = base_url

all_rows = []

while current_url:
    response = requests.get(current_url)
    soup = BeautifulSoup(response.text, "html.parser")

    quotes = soup.find_all("div", class_="quote")

    for q in quotes:
        text = q.find("span", class_="text").get_text(strip=True)
        author = q.find("small", class_="author").get_text(strip=True)
        tags = [t.get_text(strip=True) for t in q.find_all("a", class_="tag")]

        all_rows.append({
            "text": text,
            "author": author,
            "tags": ", ".join(tags)
        })

    # البحث عن زر Next
    next_btn = soup.find("li", class_="next")
    if next_btn:
        next_link = next_btn.find("a")["href"]
        current_url = base_url + next_link
    else:
        current_url = None

print("Total quotes scraped:", len(all_rows))

with open("quotes2.csv", "w", newline="", encoding="utf-8") as f:
    writer = csv.DictWriter(f, fieldnames=["text", "author", "tags"])
    writer.writeheader()
    writer.writerows(all_rows)

print("Saved file: quotes2.csv")

print(all_rows[0])
print(all_rows[-1])

import requests
from bs4 import BeautifulSoup

url = "https://dataquestio.github.io/web-scraping-pages/simple.html"
res = requests.get(url)

soup = BeautifulSoup(res.text, "html.parser")
p_text = soup.find("p").get_text(strip=True)

with open("simple_output.txt", "w", encoding="utf-8") as f:
    f.write(p_text)

print("Done:", p_text)

import requests
from bs4 import BeautifulSoup

url = "https://sites.google.com/view/cisb5123/home"
page = requests.get(url)
data = BeautifulSoup(page.content, "html.parser")

title = data.find('h1').strong.get_text() if data.find('h1') and data.find('h1').strong else "Not found"
subtitle = data.find('h2').span.get_text() if data.find('h2') and data.find('h2').span else "Not found"
intro = data.find('p').span.get_text() if data.find('p') and data.find('p').span else "Not found"

step_elements = data.find_all('li', class_='TYR86d zfr3Q')

with open("extracted_data2.txt", "w", encoding="utf-8") as file:
    file.write(f"Title: {title}\n")
    file.write(f"Subtitle: {subtitle}\n")
    file.write(f"Introduction: {intro}\n")
    file.write("Steps:\n")
    for step_element in step_elements:
        step = step_element.find('p', class_='CDt4Ke zfr3Q')
        if step:
            file.write(f"- {step.get_text(strip=True)}\n")

print("Saved: extracted_data2.txt")

import requests
from bs4 import BeautifulSoup
import csv

base_url = "https://quotes.toscrape.com"
current_url = base_url

einstein_quotes = []

while current_url:
    response = requests.get(current_url)
    soup = BeautifulSoup(response.text, "html.parser")

    quotes = soup.find_all("div", class_="quote")

    for q in quotes:
        text = q.find("span", class_="text").get_text(strip=True)
        author = q.find("small", class_="author").get_text(strip=True)
        tags = [t.get_text(strip=True) for t in q.find_all("a", class_="tag")]

        if author == "Albert Einstein":
            einstein_quotes.append({
                "text": text,
                "author": author,
                "tags": ", ".join(tags)
            })

    next_btn = soup.find("li", class_="next")
    if next_btn:
        next_link = next_btn.find("a")["href"]
        current_url = base_url + next_link
    else:
        current_url = None

print("Total Einstein quotes:", len(einstein_quotes))

with open("quotes_Einstein.csv", "w", newline="", encoding="utf-8") as f:
    writer = csv.DictWriter(f, fieldnames=["text", "author", "tags"])
    writer.writeheader()
    writer.writerows(einstein_quotes)

print("Saved: quotes_Einstein.csv")

base_url = "https://quotes.toscrape.com"
current_url = base_url

life_quotes = []

while current_url:
    response = requests.get(current_url)
    soup = BeautifulSoup(response.text, "html.parser")

    quotes = soup.find_all("div", class_="quote")

    for q in quotes:
        text = q.find("span", class_="text").get_text(strip=True)
        author = q.find("small", class_="author").get_text(strip=True)
        tags = [t.get_text(strip=True) for t in q.find_all("a", class_="tag")]

        if "life" in text.lower():
            life_quotes.append({
                "text": text,
                "author": author,
                "tags": ", ".join(tags)
            })

    next_btn = soup.find("li", class_="next")
    if next_btn:
        next_link = next_btn.find("a")["href"]
        current_url = base_url + next_link
    else:
        current_url = None

print("Total quotes containing 'life':", len(life_quotes))

with open("life_quotes.csv", "w", newline="", encoding="utf-8") as f:
    writer = csv.DictWriter(f, fieldnames=["text", "author", "tags"])
    writer.writeheader()
    writer.writerows(life_quotes)

print("Saved: life_quotes.csv")

print("Einstein total:", len(einstein_quotes))
print("Life total:", len(life_quotes))